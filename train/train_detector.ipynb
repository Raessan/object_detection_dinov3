{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fde957-6f6a-4af0-9698-97c66f9edb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_coco import DatasetCOCO\n",
    "from src.model_backbone import DinoBackbone\n",
    "from src.model_head import DinoFCOSHead\n",
    "from src.loss import compute_loss\n",
    "import config.config as cfg\n",
    "from src.common import tensor_to_image\n",
    "from src.utils import decode_outputs, plot_detections\n",
    "from collate_fn import collate_fn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d270a64-526e-4120-93eb-a124f22ee052",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "################ LOAD ALL THE PARAMETERS #############################\n",
    "# DATASET PARAMETERS\n",
    "COCO_ROOT = cfg.COCO_ROOT # Root to the folder with the prepared data\n",
    "IMG_SIZE = cfg.IMG_SIZE\n",
    "PATCH_SIZE = cfg.PATCH_SIZE\n",
    "PROB_AUGMENT_TRAINING = cfg.PROB_AUGMENT_TRAINING\n",
    "PROB_AUGMENT_VALID = cfg.PROB_AUGMENT_VALID\n",
    "IMG_MEAN = cfg.IMG_MEAN\n",
    "IMG_STD = cfg.IMG_STD\n",
    "    \n",
    "# MODEL PARAMETERS\n",
    "DINOV3_DIR = cfg.DINOV3_DIR\n",
    "FPN_CH = cfg.FPN_CH\n",
    "DINO_MODEL = cfg.DINO_MODEL\n",
    "DINO_WEIGHTS = cfg.DINO_WEIGHTS\n",
    "MODEL_TO_NUM_LAYERS = cfg.MODEL_TO_NUM_LAYERS\n",
    "MODEL_TO_EMBED_DIM = cfg.MODEL_TO_EMBED_DIM\n",
    "N_LAYERS_UNFREEZE = cfg.N_LAYERS_UNFREEZE\n",
    "N_CONVS = cfg.N_CONVS\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "BATCH_SIZE = cfg.BATCH_SIZE\n",
    "FOCAL_ALPHA = cfg.FOCAL_ALPHA\n",
    "FOCAL_GAMMA = cfg.FOCAL_GAMMA\n",
    "WEIGHT_REG = cfg.WEIGHT_REG\n",
    "WEIGHT_CTR = cfg.WEIGHT_CTR\n",
    "\n",
    "LEARNING_RATE = cfg.LEARNING_RATE\n",
    "WEIGHT_DECAY = cfg.WEIGHT_DECAY\n",
    "NUM_EPOCHS = cfg.NUM_EPOCHS\n",
    "NUM_SAMPLES_PLOT = cfg.NUM_SAMPLES_PLOT\n",
    "\n",
    "LOAD_MODEL = cfg.LOAD_MODEL\n",
    "SAVE_MODEL = cfg.SAVE_MODEL\n",
    "MODEL_PATH_TRAIN_LOAD = cfg.MODEL_PATH_TRAIN_LOAD\n",
    "RESULTS_PATH = cfg.RESULTS_PATH\n",
    "\n",
    "# INFERENCE PARAMETERS (FOR PLOTTING)\n",
    "SCORE_THRESH = cfg.SCORE_THRESH\n",
    "NMS_THRESH = cfg.NMS_THRESH\n",
    "\n",
    "train_set = DatasetCOCO(COCO_ROOT, \"train\", IMG_SIZE, PATCH_SIZE, PROB_AUGMENT_TRAINING, IMG_MEAN, IMG_STD)\n",
    "train_dataloader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_set = DatasetCOCO(COCO_ROOT, \"val\", IMG_SIZE, PATCH_SIZE, PROB_AUGMENT_VALID, IMG_MEAN, IMG_STD)\n",
    "val_dataloader = DataLoader(val_set, batch_size = BATCH_SIZE, num_workers=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "num_classes = len(train_set.class_names)\n",
    "\n",
    "dino_model = torch.hub.load(\n",
    "        repo_or_dir=DINOV3_DIR,\n",
    "        model=DINO_MODEL,\n",
    "        source=\"local\",\n",
    "        weights=DINO_WEIGHTS\n",
    ")\n",
    "n_layers_dino = MODEL_TO_NUM_LAYERS[DINO_MODEL]\n",
    "embed_dim = MODEL_TO_EMBED_DIM[DINO_MODEL]\n",
    "\n",
    "dino_backbone = DinoBackbone(dino_model, n_layers_dino).to(device)\n",
    "model_head = DinoFCOSHead(backbone_out_channels=embed_dim, fpn_channels=FPN_CH, num_classes=num_classes, num_convs=N_CONVS).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_head.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Load model\n",
    "if LOAD_MODEL:\n",
    "    model_head.load_state_dict(torch.load(MODEL_PATH_TRAIN_LOAD))\n",
    "    print(\"Model successfully loaded!\")\n",
    "\n",
    "\n",
    "# Freeze parameters\n",
    "for p in dino_backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze last N transformer blocks\n",
    "if N_LAYERS_UNFREEZE > 0:\n",
    "    # Unfreeze the last norm layer\n",
    "    for p in dino_backbone.dino.norm.parameters():\n",
    "        p.requires_grad = True\n",
    "    for block in dino_backbone.dino.blocks[-N_LAYERS_UNFREEZE:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "n_params = sum([p.numel() for p in dino_backbone.parameters()]) + sum([p.numel() for p in model_head.parameters()])\n",
    "print(\"Total number of parameters: \", n_params)\n",
    "n_trainable_params = sum([p.numel() for p in dino_backbone.parameters() if p.requires_grad]) + sum([p.numel() for p in model_head.parameters() if p.requires_grad])\n",
    "print(\"Total number of trainable parameters: \", n_trainable_params)\n",
    "n_params_backbone = sum([p.numel() for p in dino_backbone.parameters()])\n",
    "print(\"Number parameters backbone: \", n_params_backbone)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    folder_path = f\"{RESULTS_PATH}/{current_date}\"\n",
    "    \n",
    "    json_params = { \n",
    "        \"IMG_SIZE\" : IMG_SIZE, \n",
    "        \"PATCH_SIZE\" : PATCH_SIZE, \n",
    "        \"PROB_AUGMENT_TRAINING\": PROB_AUGMENT_TRAINING,\n",
    "        \"PROB_AUGMENT_VALID\": PROB_AUGMENT_VALID,\n",
    "        \"DINO_MODEL\": DINO_MODEL,\n",
    "        \"N_LAYERS_UNFREEZE\": N_LAYERS_UNFREEZE,\n",
    "        \"N_CONVS\": N_CONVS,\n",
    "        \"FPN_CH\": FPN_CH,\n",
    "        \"FOCAL_ALPHA\" : FOCAL_ALPHA,\n",
    "        \"FOCAL_GAMMA\" : FOCAL_GAMMA,\n",
    "        \"WEIGHT_REG\" : WEIGHT_REG,\n",
    "        \"WEIGHT_CTR\": WEIGHT_CTR,\n",
    "        \"LEARNING_RATE\" : LEARNING_RATE,\n",
    "        \"LOAD_MODEL\" : LOAD_MODEL,\n",
    "        \"MODEL_PATH_TRAIN_LOAD\" : MODEL_PATH_TRAIN_LOAD,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe44f1-6d7d-41b3-8ef5-822018eb054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO backbone always in eval mode\n",
    "dino_backbone.eval()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##################### TRAIN #######################\n",
    "    model_head.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, boxes, labels) in enumerate(tqdm(train_dataloader)):\n",
    "        images = images.to(device, dtype=torch.float) #, boxes.to(device, dtype=torch.float), labels.to(device, dtype=torch.int)\n",
    "        boxes = [box.to(device, dtype=torch.float) for box in boxes]\n",
    "        labels = [label.to(device, dtype=torch.int) for label in labels]\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        feat = dino_backbone(images)\n",
    "        outputs = model_head(feat)\n",
    "\n",
    "        first_stride = IMG_SIZE / outputs['cls'][0].shape[2]\n",
    "        strides = [first_stride]\n",
    "        for l in range(1,len(outputs['cls'])):\n",
    "            strides.append(first_stride*2**l)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(outputs, boxes, labels, images.shape[2:], strides, FOCAL_ALPHA, FOCAL_GAMMA, WEIGHT_REG, WEIGHT_CTR, \n",
    "                           scale_ranges = None)\n",
    "\n",
    "        \n",
    "        # Backward pass\n",
    "        loss[0].backward()\n",
    "        \n",
    "        # Gradient clipping and Optimize\n",
    "        # clip all gradients to max norm 5.0\n",
    "        torch.nn.utils.clip_grad_norm_(model_head.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss[0].item()\n",
    "\n",
    "        # Show error\n",
    "        if (batch_idx % 200 == 0 and batch_idx > 0):\n",
    "            print(f\"Epoch {epoch+1}, batch {batch_idx}, Loss: {train_loss/(batch_idx+1)}, cls loss: {loss[1].item()}, regression loss: {loss[2].item()}, ctr loss: {loss[3].item()}\")\n",
    "\n",
    "        # Plot\n",
    "        if (batch_idx % 2000 == 0 and batch_idx > 0):\n",
    "            for i in range(NUM_SAMPLES_PLOT):\n",
    "                sample_cls = []\n",
    "                sample_reg = []\n",
    "                sample_ctr = []\n",
    "                for j in range(len(outputs['cls'])):\n",
    "                    sample_cls.append(outputs[\"cls\"][j][i].unsqueeze(0))\n",
    "                    sample_reg.append(outputs[\"reg\"][j][i].unsqueeze(0))\n",
    "                    sample_ctr.append(outputs[\"ctr\"][j][i].unsqueeze(0))\n",
    "                output_sample = {}\n",
    "                output_sample[\"cls\"] = sample_cls\n",
    "                output_sample[\"reg\"] = sample_reg\n",
    "                output_sample[\"ctr\"] = sample_ctr\n",
    "                boxes_plot, scores_plot, labels_plot = decode_outputs(output_sample, images[i].shape[1:], strides, score_thresh=SCORE_THRESH, nms_thresh=NMS_THRESH)\n",
    "                # Now we create targets\n",
    "                box_target = torch.zeros_like(boxes[i])\n",
    "                if box_target.ndim>1:\n",
    "                    box_target[:, 0] = boxes[i][:, 0] * images[i].shape[2]\n",
    "                    box_target[:, 1] = boxes[i][:, 1] * images[i].shape[1]\n",
    "                    box_target[:, 2] = box_target[:, 0] + boxes[i][:, 2] * images[i].shape[2]\n",
    "                    box_target[:, 3] = box_target[:, 1] + boxes[i][:, 3] * images[i].shape[1]\n",
    "                    plot_detections(tensor_to_image(images[i], IMG_MEAN, IMG_STD), box_target.cpu(), torch.ones_like(labels[i]).cpu(), labels[i].cpu(), train_set.class_names, figsize=(5,5))\n",
    "                plot_detections(tensor_to_image(images[i], IMG_MEAN, IMG_STD), boxes_plot.cpu(), scores_plot.cpu(), labels_plot.cpu(), val_set.class_names, figsize=(5,5))\n",
    "    train_loss /= float(batch_idx+1)\n",
    "    \n",
    "    \n",
    "    ##################### VALIDATION #######################\n",
    "    model_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loss_cls = 0.0\n",
    "    val_loss_reg = 0.0\n",
    "    val_loss_ctr = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, boxes, labels) in enumerate(tqdm(val_dataloader)):\n",
    "            images = images.to(device, dtype=torch.float) #, boxes.to(device, dtype=torch.float), labels.to(device, dtype=torch.int)\n",
    "            boxes = [box.to(device, dtype=torch.float) for box in boxes]\n",
    "            labels = [label.to(device, dtype=torch.int) for label in labels]\n",
    "            \n",
    "            # Forward pass\n",
    "            feat = dino_backbone(images)\n",
    "            outputs = model_head(feat)\n",
    "    \n",
    "            first_stride = IMG_SIZE / outputs['cls'][0].shape[2]\n",
    "            strides = [first_stride]\n",
    "            for l in range(1,len(outputs['cls'])):\n",
    "                strides.append(first_stride*2**l)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = compute_loss(outputs, boxes, labels, images.shape[2:], strides, FOCAL_ALPHA, FOCAL_GAMMA, WEIGHT_REG, WEIGHT_CTR,\n",
    "                               scale_ranges = None)\n",
    "    \n",
    "            val_loss += loss[0].item()\n",
    "            val_loss_cls += loss[1].item()\n",
    "            val_loss_reg += loss[2].item()\n",
    "            val_loss_ctr += loss[3].item()\n",
    "\n",
    "            # Plot\n",
    "            if (batch_idx==0):\n",
    "                for i in range(NUM_SAMPLES_PLOT):\n",
    "                    sample_cls = []\n",
    "                    sample_reg = []\n",
    "                    sample_ctr = []\n",
    "                    for j in range(len(outputs['cls'])):\n",
    "                        sample_cls.append(outputs[\"cls\"][j][i].unsqueeze(0))\n",
    "                        sample_reg.append(outputs[\"reg\"][j][i].unsqueeze(0))\n",
    "                        sample_ctr.append(outputs[\"ctr\"][j][i].unsqueeze(0))\n",
    "                    output_sample = {}\n",
    "                    output_sample[\"cls\"] = sample_cls\n",
    "                    output_sample[\"reg\"] = sample_reg\n",
    "                    output_sample[\"ctr\"] = sample_ctr\n",
    "                    boxes_plot, scores_plot, labels_plot = decode_outputs(output_sample, images[i].shape[1:], strides, score_thresh=SCORE_THRESH, nms_thresh=NMS_THRESH)\n",
    "                    # Now we create targets\n",
    "                    box_target = torch.zeros_like(boxes[i])\n",
    "                    if box_target.ndim>1:\n",
    "                        box_target[:, 0] = boxes[i][:, 0] * images[i].shape[2]\n",
    "                        box_target[:, 1] = boxes[i][:, 1] * images[i].shape[1]\n",
    "                        box_target[:, 2] = box_target[:, 0] + boxes[i][:, 2] * images[i].shape[2]\n",
    "                        box_target[:, 3] = box_target[:, 1] + boxes[i][:, 3] * images[i].shape[1]\n",
    "                        plot_detections(tensor_to_image(images[i], IMG_MEAN, IMG_STD), box_target.cpu(), torch.ones_like(labels[i]).cpu(), labels[i].cpu(), train_set.class_names, figsize=(5,5))\n",
    "                    plot_detections(tensor_to_image(images[i], IMG_MEAN, IMG_STD), boxes_plot.cpu(), scores_plot.cpu(), labels_plot.cpu(), val_set.class_names, figsize=(5,5))\n",
    "                    \n",
    "\n",
    "        val_loss /= float(batch_idx+1)\n",
    "        val_loss_cls /= float(batch_idx+1)\n",
    "        val_loss_reg /= float(batch_idx+1)\n",
    "        val_loss_ctr /= float(batch_idx+1)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss}, val loss NN total: {val_loss}, val loss CLS: {val_loss_cls},  val loss Reg: {val_loss_reg}, val loss ctr: {val_loss_ctr}\")\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            # Save model and params\n",
    "            json_params_epoch = json_params.copy()\n",
    "            json_params_epoch[\"epoch\"] = epoch\n",
    "            json_params_epoch[\"train_loss\"] = train_loss\n",
    "            json_params_epoch[\"val_loss\"] = val_loss\n",
    "            json_params_epoch[\"val_loss_cls\"] = val_loss_cls\n",
    "            json_params_epoch[\"val_loss_reg\"] = val_loss_reg\n",
    "            json_params_epoch[\"val_loss_ctr\"] = val_loss_ctr\n",
    "            model_path = os.path.join(folder_path,f\"model_{epoch}.pth\")\n",
    "            json_path = os.path.join(folder_path,f\"params_{epoch}.json\")\n",
    "            torch.save(model_head.state_dict(), model_path)\n",
    "            with open(json_path, \"w\") as outfile:\n",
    "                json.dump(json_params_epoch, outfile)\n",
    "    \n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4e188-61d0-4d11-ac81-ad7d88b40534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
