{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fde957-6f6a-4af0-9698-97c66f9edb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_coco import DatasetCOCO\n",
    "from src.model_backbone import DinoBackbone\n",
    "from src.model_head import DinoFCOSHead\n",
    "from src.loss import compute_loss\n",
    "#from src.draw_samples_training import draw_samples_training\n",
    "import config.config as cfg\n",
    "from src.utils import collate_fn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d270a64-526e-4120-93eb-a124f22ee052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n",
      "loading annotations into memory...\n",
      "Done (t=7.50s)\n",
      "creating index...\n",
      "index created!\n",
      "Total number of classes: 80\n",
      "loading annotations into memory...\n",
      "Done (t=1.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Total number of classes: 80\n",
      "Total number of parameters:  33235477\n",
      "Total number of trainable parameters:  4538005\n",
      "Number parameters backbone:  28697472\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "################ LOAD ALL THE PARAMETERS #############################\n",
    "# DATASET PARAMETERS\n",
    "COCO_ROOT = cfg.COCO_ROOT # Root to the folder with the prepared data\n",
    "IMG_SIZE = cfg.IMG_SIZE\n",
    "PATCH_SIZE = cfg.PATCH_SIZE\n",
    "PROB_AUGMENT_TRAINING = cfg.PROB_AUGMENT_TRAINING\n",
    "PROB_AUGMENT_VALID = cfg.PROB_AUGMENT_VALID\n",
    "IMG_MEAN = cfg.IMG_MEAN\n",
    "IMG_STD = cfg.IMG_STD\n",
    "    \n",
    "# MODEL PARAMETERS\n",
    "DINOV3_DIR = cfg.DINOV3_DIR\n",
    "FPN_CH = cfg.FPN_CH\n",
    "DINO_MODEL = cfg.DINO_MODEL\n",
    "MODEL_TO_NUM_LAYERS = cfg.MODEL_TO_NUM_LAYERS\n",
    "MODEL_TO_EMBED_DIM = cfg.MODEL_TO_EMBED_DIM\n",
    "N_LAYERS_UNFREEZE = cfg.N_LAYERS_UNFREEZE\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "BATCH_SIZE = cfg.BATCH_SIZE\n",
    "FOCAL_ALPHA = cfg.FOCAL_ALPHA\n",
    "FOCAL_GAMMA = cfg.FOCAL_GAMMA\n",
    "WEIGHT_REG = cfg.WEIGHT_REG\n",
    "WEIGHT_CTR = cfg.WEIGHT_CTR\n",
    "\n",
    "LEARNING_RATE = cfg.LEARNING_RATE\n",
    "NUM_EPOCHS = cfg.NUM_EPOCHS\n",
    "NUM_SAMPLES_PLOT = cfg.NUM_SAMPLES_PLOT\n",
    "\n",
    "LOAD_MODEL = cfg.LOAD_MODEL\n",
    "SAVE_MODEL = cfg.SAVE_MODEL\n",
    "MODEL_PATH_TRAIN_LOAD = cfg.MODEL_PATH_TRAIN_LOAD\n",
    "RESULTS_PATH = cfg.RESULTS_PATH\n",
    "\n",
    "train_set = DatasetCOCO(COCO_ROOT, \"train\", IMG_SIZE, PATCH_SIZE, PROB_AUGMENT_TRAINING, IMG_MEAN, IMG_STD)\n",
    "train_dataloader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_set = DatasetCOCO(COCO_ROOT, \"val\", IMG_SIZE, PATCH_SIZE, PROB_AUGMENT_VALID, IMG_MEAN, IMG_STD)\n",
    "val_dataloader = DataLoader(val_set, batch_size = BATCH_SIZE, num_workers=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "num_classes = len(train_set.class_names)\n",
    "\n",
    "dino_model = torch.hub.load(\n",
    "        repo_or_dir=DINOV3_DIR,\n",
    "        model=\"dinov3_vits16plus\",\n",
    "        source=\"local\"\n",
    ")\n",
    "n_layers_dino = MODEL_TO_NUM_LAYERS[DINO_MODEL]\n",
    "embed_dim = MODEL_TO_EMBED_DIM[DINO_MODEL]\n",
    "\n",
    "dino_backbone = DinoBackbone(dino_model, n_layers_dino).to(device)\n",
    "model_head = DinoFCOSHead(backbone_out_channels=embed_dim, fpn_channels=FPN_CH, num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_head.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Load model\n",
    "if LOAD_MODEL:\n",
    "    model_head.load_state_dict(torch.load(MODEL_PATH_TRAIN_LOAD))\n",
    "    print(\"Model successfully loaded!\")\n",
    "\n",
    "\n",
    "# Freeze parameters\n",
    "for p in dino_backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze last N transformer blocks\n",
    "if N_LAYERS_UNFREEZE > 0:\n",
    "    # Unfreeze the last norm layer\n",
    "    for p in dino_backbone.dino.norm.parameters():\n",
    "        p.requires_grad = True\n",
    "    for block in dino_backbone.dino.blocks[-N_LAYERS_UNFREEZE:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "n_params = sum([p.numel() for p in dino_backbone.parameters()]) + sum([p.numel() for p in model_head.parameters()])\n",
    "print(\"Total number of parameters: \", n_params)\n",
    "n_trainable_params = sum([p.numel() for p in dino_backbone.parameters() if p.requires_grad]) + sum([p.numel() for p in model_head.parameters() if p.requires_grad])\n",
    "print(\"Total number of trainable parameters: \", n_trainable_params)\n",
    "n_params_backbone = sum([p.numel() for p in dino_backbone.parameters()])\n",
    "print(\"Number parameters backbone: \", n_params_backbone)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    folder_path = f\"{RESULTS_PATH}/{current_date}\"\n",
    "    \n",
    "    json_params = { \n",
    "        \"IMG_SIZE\" : IMG_SIZE, \n",
    "        \"PATCH_SIZE\" : PATCH_SIZE, \n",
    "        \"PROB_AUGMENT_TRAINING\": PROB_AUGMENT_TRAINING,\n",
    "        \"PROB_AUGMENT_VALID\": PROB_AUGMENT_VALID,\n",
    "        \"DINO_MODEL\": DINO_MODEL,\n",
    "        \"N_LAYERS_UNFREEZE\": N_LAYERS_UNFREEZE,\n",
    "        \"FPN_CH\": FPN_CH,\n",
    "        \"FOCAL_ALPHA\" : FOCAL_ALPHA,\n",
    "        \"FOCAL_GAMMA\" : FOCAL_GAMMA,\n",
    "        \"WEIGHT_REG\" : WEIGHT_REG,\n",
    "        \"WEIGHT_CTR\": WEIGHT_CTR,\n",
    "        \"LEARNING_RATE\" : LEARNING_RATE,\n",
    "        \"LOAD_MODEL\" : LOAD_MODEL,\n",
    "        \"MODEL_PATH_TRAIN_LOAD\" : MODEL_PATH_TRAIN_LOAD,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe44f1-6d7d-41b3-8ef5-822018eb054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▎                      | 201/3697 [02:37<44:52,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 200, Loss: 1.5401874098611708, cls loss: 0.4163675904273987, regression loss: 0.9999523162841797, ctr loss: 7.203803397715092e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██▌                     | 401/3697 [05:12<43:20,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 400, Loss: 1.4584570926918354, cls loss: 0.3413376212120056, regression loss: 0.9937861561775208, ctr loss: 0.00013686894089914858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███▉                    | 601/3697 [07:49<39:29,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 600, Loss: 1.4171153473576372, cls loss: 0.3267313241958618, regression loss: 0.9886676073074341, ctr loss: 0.00012433268420863897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████▏                  | 801/3697 [10:24<37:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 800, Loss: 1.391193706742238, cls loss: 0.3618829548358917, regression loss: 0.9837740063667297, ctr loss: 0.00010283814481226727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████▍                  | 836/3697 [10:51<37:27,  1.27it/s]"
     ]
    }
   ],
   "source": [
    "# DINO backbone always in eval mode\n",
    "dino_backbone.eval()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##################### TRAIN #######################\n",
    "    model_head.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (image, boxes, labels) in enumerate(tqdm(train_dataloader)):\n",
    "        image = image.to(device, dtype=torch.float) #, boxes.to(device, dtype=torch.float), labels.to(device, dtype=torch.int)\n",
    "        boxes = [box.to(device, dtype=torch.float) for box in boxes]\n",
    "        labels = [label.to(device, dtype=torch.int) for label in labels]\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        feat = dino_backbone(image)\n",
    "        outputs = model_head(feat)\n",
    "\n",
    "        first_stride = IMG_SIZE / outputs['cls'][0].shape[2]\n",
    "        strides = [first_stride, first_stride*2, first_stride*4]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(outputs, boxes, labels, image.shape[2:], strides, FOCAL_ALPHA, FOCAL_GAMMA, WEIGHT_REG, WEIGHT_CTR)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss[0].backward()\n",
    "        \n",
    "        # Gradient clipping and Optimize\n",
    "        # clip all gradients to max norm 5.0\n",
    "        torch.nn.utils.clip_grad_norm_(model_head.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss[0].item()\n",
    "\n",
    "        if (batch_idx % 200 == 0 and batch_idx > 0):\n",
    "            print(f\"Epoch {epoch+1}, batch {batch_idx}, Loss: {train_loss/(batch_idx+1)}, cls loss: {loss[1].item()}, regression loss: {loss[2].item()}, ctr loss: {loss[3].item()}\")\n",
    "\n",
    "        #if (batch_idx % 10000 == 0 and batch_idx > 0):\n",
    "            #draw_samples_training(template, search, torch.sigmoid(pred_heatmap), pred_bbox, heatmap, bbox, train_set.mean, train_set.std, THRESHOLD_CLS, NUM_SAMPLES_PLOT, video_template_name, video_search_name)\n",
    "    train_loss /= float(batch_idx+1)\n",
    "    \n",
    "    ##################### VALIDATION #######################\n",
    "    model_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loss_cls = 0.0\n",
    "    val_loss_reg = 0.0\n",
    "    val_loss_ctr = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, boxes, labels) in enumerate(tqdm(val_dataloader)):\n",
    "            image = image.to(device, dtype=torch.float) #, boxes.to(device, dtype=torch.float), labels.to(device, dtype=torch.int)\n",
    "            boxes = [box.to(device, dtype=torch.float) for box in boxes]\n",
    "            labels = [label.to(device, dtype=torch.int) for label in labels]\n",
    "            \n",
    "            # Forward pass\n",
    "            feat = dino_backbone(image)\n",
    "            outputs = model_head(feat)\n",
    "    \n",
    "            first_stride = IMG_SIZE / outputs['cls'][0].shape[2]\n",
    "            strides = [first_stride, first_stride*2, first_stride*4]\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = compute_loss(outputs, boxes, labels, image.shape[2:], strides, FOCAL_ALPHA, FOCAL_GAMMA, WEIGHT_REG, WEIGHT_CTR)\n",
    "    \n",
    "            val_loss += loss[0].item()\n",
    "            val_loss_cls += loss[1].item()\n",
    "            val_loss_reg += loss[2].item()\n",
    "            val_loss_ctr += loss[3].item()\n",
    "    \n",
    "            #if (batch_idx == 0):\n",
    "                #draw_samples_training(template, search, torch.sigmoid(pred_heatmap), pred_bbox, heatmap, bbox, train_set.mean, train_set.std, THRESHOLD_CLS, NUM_SAMPLES_PLOT, video_template_name, video_search_name)\n",
    "    \n",
    "        val_loss /= float(batch_idx+1)\n",
    "        val_loss_cls /= float(batch_idx+1)\n",
    "        val_loss_reg /= float(batch_idx+1)\n",
    "        val_loss_ctr /= float(batch_idx+1)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss}, val loss NN total: {val_loss}, val loss CLS: {val_loss_cls},  val loss Reg: {val_loss_reg}, val loss ctr: {val_loss_ctr}\")\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            # Save model and params\n",
    "            json_params_epoch = json_params.copy()\n",
    "            json_params_epoch[\"epoch\"] = epoch\n",
    "            json_params_epoch[\"train_loss\"] = train_loss\n",
    "            json_params_epoch[\"val_loss\"] = val_loss\n",
    "            json_params_epoch[\"val_loss_cls\"] = val_loss_cls\n",
    "            json_params_epoch[\"val_loss_reg\"] = val_loss_reg\n",
    "            json_params_epoch[\"val_loss_ctr\"] = val_loss_ctr\n",
    "            model_path = os.path.join(folder_path,f\"model_{epoch}.pth\")\n",
    "            json_path = os.path.join(folder_path,f\"params_{epoch}.json\")\n",
    "            torch.save(model_head.state_dict(), model_path)\n",
    "            with open(json_path, \"w\") as outfile:\n",
    "                json.dump(json_params_epoch, outfile)\n",
    "    \n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4e188-61d0-4d11-ac81-ad7d88b40534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
